{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting by building the game and the user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showBoard(board):\n",
    "    '''Shows the board'''\n",
    "    for val in board:\n",
    "        print(val)\n",
    "\n",
    "def checkFree(x,board):\n",
    "    '''Takes an x,y postion and checks if that point on the board is free'''\n",
    "    b = board.reshape(1,9)\n",
    "    if b[0][x] ==0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def checkWin(toggle, board):\n",
    "    '''checks wins for each player for diagonals, rows, columns'''\n",
    "    if toggle:\n",
    "        high = 6\n",
    "    else:\n",
    "        high = 15\n",
    "    if board.diagonal().sum() == high:\n",
    "        return True\n",
    "    if np.flip(board,1).diagonal().sum() == high:\n",
    "        return True\n",
    "    for val in range(0,3):\n",
    "        if board[val,:].sum()==high:\n",
    "            return True\n",
    "    for val in range(0,3):\n",
    "        if board[:,val].sum()==high:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getAvailablePositions(board):\n",
    "    pos = []\n",
    "    for i in range(9):\n",
    "        if checkFree(i,board):\n",
    "            pos.append(i)\n",
    "    return pos\n",
    "\n",
    "def placePiece(x,y,nought_or_cross,board):\n",
    "    '''Takes a x, y position and a X or O with X=1, and O=2'''\n",
    "    new_board = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            new_board [i][j] = board[i][j]\n",
    "    \n",
    "    new_board[x,y] = nought_or_cross\n",
    "    return new_board\n",
    "\n",
    "def getReward(result,num_moves):\n",
    "    '''Reward a game won or lost'''\n",
    "    if result == 'lost':\n",
    "        return -1+(-num_moves/9)\n",
    "    elif result == 'win':\n",
    "        return 1+(-num_moves/9)\n",
    "    else:\n",
    "        return 0+(-num_moves/9)\n",
    "    \n",
    "def getMove(action):\n",
    "    '''Gets the move based on the chosen action number'''\n",
    "    moves = [[0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\n",
    "    return moves[action]\n",
    "\n",
    "def convert_to_normal(board):\n",
    "    '''Converts the new board to a (1,9) shape and replaces the 5 and 2 with a 1 and -1'''\n",
    "    b = np.zeros((1,9))\n",
    "    count = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            b[0][count] = board[i][j]\n",
    "            count+=1\n",
    "    for i in range(9):\n",
    "        if b[0][i]==5:\n",
    "            b[0][i] = 1\n",
    "        elif b[0][i]==2:\n",
    "            b[0][i] = -1\n",
    "    return b\n",
    "\n",
    "def convert_to_normal_diff(board):\n",
    "    '''Converts the new board to a (1,9) shape and replaces the 5 and 2 with a 1 and -1'''\n",
    "    b = np.zeros((9,))\n",
    "    count = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            b[count] = board[i][j]\n",
    "            count+=1\n",
    "    for i in range(9):\n",
    "        if b[i]==5:\n",
    "            b[i] = 1\n",
    "        elif b[i]==2:\n",
    "            b[i] = -1\n",
    "    return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createBrain():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(164, kernel_initializer='lecun_uniform', input_shape=(9,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2)) #I'm not using dropout, but maybe you wanna give it a try?\n",
    "\n",
    "    model.add(Dense(150, kernel_initializer='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(9, kernel_initializer='lecun_uniform'))\n",
    "    model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "    #rms = RMSprop()\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 0\n",
      "Game #: 10\n",
      "Game #: 20\n",
      "Game #: 30\n",
      "Game #: 40\n",
      "Game #: 50\n",
      "Game #: 60\n",
      "Game #: 70\n",
      "Game #: 80\n",
      "Game #: 90\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#run function\n",
    "\n",
    "epsilon = 1\n",
    "gamma = 0.9\n",
    "epochs = 100000\n",
    "batchSize = 40\n",
    "buffer = 80\n",
    "replay = []\n",
    "replay2 = []\n",
    "\n",
    "model1 = createBrain()\n",
    "model2 = createBrain()\n",
    "\n",
    "h = 0\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    \n",
    "    toggle = False\n",
    "    board = np.zeros((3,3))\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    result = 'play'\n",
    "    if epoch%1000 == 0:\n",
    "        print(\"Game #: %s\" % (epoch,))\n",
    "    while result != 'lose' and result != 'win' and result !='draw':\n",
    "        toggle = not toggle\n",
    "        if toggle == True:\n",
    "            qval = model1.predict(convert_to_normal(board), batch_size=1)\n",
    "            if epsilon > random.random():\n",
    "                action = np.random.choice(getAvailablePositions(board))\n",
    "            else:\n",
    "                #print(getAvailablePositions(board))\n",
    "                action = getAvailablePositions(board)[np.argmax(qval[0][getAvailablePositions(board)])]\n",
    "            move = getMove(action)\n",
    "            x = move[0]\n",
    "            y = move[1]\n",
    "            n_or_c = 5\n",
    "        else:\n",
    "            qval = model2.predict(convert_to_normal(board), batch_size=1)\n",
    "            if epsilon > random.random():\n",
    "                action = np.random.choice(getAvailablePositions(board))\n",
    "            else:\n",
    "                #print(getAvailablePositions(board))\n",
    "                action = getAvailablePositions(board)[np.argmax(qval[0][getAvailablePositions(board)])]\n",
    "            move = getMove(action)\n",
    "            x = move[0]\n",
    "            y = move[1]\n",
    "            n_or_c = 2\n",
    "        \n",
    "        new_board = placePiece(int(x),int(y),n_or_c, board)\n",
    "        \n",
    "        if checkWin(toggle, new_board):\n",
    "             result = 'lose'\n",
    "        elif checkWin(not toggle, new_board):\n",
    "            result = 'win'\n",
    "        elif not getAvailablePositions(new_board):\n",
    "            result = 'draw'\n",
    "        \n",
    "        if toggle == True:\n",
    "            reward = getReward(result, count1)\n",
    "            if (len(replay) < buffer): #if buffer not filled, add to it\n",
    "                replay.append((board, action, reward, new_board))\n",
    "            else:\n",
    "                if (h < (buffer-1)):\n",
    "                    h += 1\n",
    "                else:\n",
    "                    h = 0\n",
    "                replay[h] = (board, action, reward, new_board)\n",
    "                #randomly sample our experience replay memory\n",
    "                minibatch = random.sample(replay, batchSize)\n",
    "                X_train = []\n",
    "                y_train = []\n",
    "                for memory in minibatch:\n",
    "                    state, action, reward, new_state = memory\n",
    "                    old_qval = model1.predict(convert_to_normal(state), batch_size=1)\n",
    "                    newQ = model1.predict(convert_to_normal(new_state), batch_size=1)\n",
    "                    maxQ = np.max(newQ)\n",
    "                    y = np.zeros((1,9))\n",
    "                    y[:] = old_qval[:]\n",
    "                    if result != 'lose' or result != 'win' or result !='draw': #non-terminal state\n",
    "                        update = (reward + (gamma * maxQ))\n",
    "                    else: #terminal state\n",
    "                        update = reward\n",
    "                    y[0][action] = update #target output\n",
    "                    X_train.append(convert_to_normal_diff(state))\n",
    "                    y_train.append(y.reshape(9,))\n",
    "                \n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "                model1.fit(X_train, y_train, batch_size=batchSize, epochs=1, verbose=0)\n",
    "            count1 = count1 + 1\n",
    "        else:\n",
    "            reward = getReward(result, count2)\n",
    "            if (len(replay2) < buffer): #if buffer not filled, add to it\n",
    "                replay2.append((board, action, reward, new_board))\n",
    "            else:\n",
    "                if (h < (buffer-1)):\n",
    "                    h += 1\n",
    "                else:\n",
    "                    h = 0\n",
    "                replay2[h] = (board, action, reward, new_board)\n",
    "                #randomly sample our experience replay memory\n",
    "                minibatch = random.sample(replay2, batchSize)\n",
    "                X_train = []\n",
    "                y_train = []\n",
    "                for memory in minibatch:\n",
    "                    state, action, reward, new_state = memory\n",
    "                    old_qval = model2.predict(convert_to_normal(state), batch_size=1)\n",
    "                    newQ = model2.predict(convert_to_normal(new_state), batch_size=1)\n",
    "                    maxQ = np.max(newQ)\n",
    "                    y = np.zeros((1,9))\n",
    "                    y[:] = old_qval[:]\n",
    "                    if result != 'lose' or result != 'win' or result !='draw': #non-terminal state\n",
    "                        update = (reward + (gamma * maxQ))\n",
    "                    else: #terminal state\n",
    "                        update = reward\n",
    "                    y[0][action] = update #target output\n",
    "                    X_train.append(convert_to_normal_diff(state))\n",
    "                    y_train.append(y.reshape(9,))\n",
    "                \n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "                model2.fit(X_train, y_train, batch_size=batchSize, epochs=1, verbose=0)\n",
    "            count2 = count2 + 1\n",
    "            \n",
    "        board = new_board\n",
    "        #showBoard(board)\n",
    "        if epsilon > 0.1:\n",
    "            epsilon -= (1/epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "[ 5.  0.  0.]\n",
      "--------------\n",
      "[ 0.  0.  2.]\n",
      "[ 0.  0.  0.]\n",
      "[ 5.  0.  0.]\n",
      "--------------\n",
      "[ 0.  0.  2.]\n",
      "[ 5.  0.  0.]\n",
      "[ 5.  0.  0.]\n",
      "--------------\n",
      "[ 0.  2.  2.]\n",
      "[ 5.  0.  0.]\n",
      "[ 5.  0.  0.]\n",
      "--------------\n",
      "5 win\n",
      "[ 5.  2.  2.]\n",
      "[ 5.  0.  0.]\n",
      "[ 5.  0.  0.]\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "toggle = False\n",
    "board = np.zeros((3,3))\n",
    "result = 'play'\n",
    "\n",
    "while result != 'lose' and result != 'win' and result !='draw':\n",
    "    toggle = not toggle\n",
    "    \n",
    "    if toggle == True:\n",
    "        qval = model1.predict(convert_to_normal(board), batch_size=1)\n",
    "        action = getAvailablePositions(board)[(np.argmax(qval[0][getAvailablePositions(board)]))]\n",
    "        move = getMove(action)\n",
    "        x = move[0]\n",
    "        y = move[1]\n",
    "        n_or_c = 5\n",
    "    else:\n",
    "        qval = model2.predict(convert_to_normal(board), batch_size=1)\n",
    "        action = getAvailablePositions(board)[(np.argmax(qval[0][getAvailablePositions(board)]))]\n",
    "        move = getMove(action)\n",
    "        x = move[0]\n",
    "        y = move[1]\n",
    "        n_or_c = 2\n",
    "    \n",
    "    new_board = placePiece(int(x),int(y),n_or_c, board)\n",
    "\n",
    "    if checkWin(toggle, new_board):\n",
    "         result = 'lose'\n",
    "    elif checkWin(not toggle, new_board):\n",
    "        result = 'win'\n",
    "        print(n_or_c,result)\n",
    "    elif not getAvailablePositions(new_board):\n",
    "        result = 'draw'\n",
    "\n",
    "    board = new_board\n",
    "    showBoard(board)\n",
    "    print('--------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As can be seen, the game is played to a draw. However there are some small issues of the players not capitalizing on the errors of the opposing player. This could be due to not enough training. As we only ran 100 Games.\n",
    "\n",
    "## Ideally we want the game to be a draw. This is the solved game state. \n",
    "\n",
    "## After 1000 Games player 2 is winning. Which is not ideal. \n",
    "\n",
    "## After 10000 games the other player is winning. I wonder if there is an issue with the way it understands the actions based on the turn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
