{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting by building the game and the user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial thoughts: Have an set of actions to choose from. i.e the available board positions and either randomly pick one or choose the one that provided the best reward in the past. This means we need to define rewards for each individual placements. But we are not taking into account the temporal element as well as the rest of the board. Is this important? I would think so. Or is there a list of optimal positions in descending order so if one is chosen then choose the next best in the list.\n",
    "\n",
    "#### Other option is to have a reward for each game: Then each set of moves made by the agent is rated. The reward per each move is then summed up and you get the distribution for each move. Then the agent can select the best moves based on the rewards.  \n",
    "\n",
    "#### This also means I have to play thousands of games to teach the agent. Otherwise I have to create a good expert. We could have an expert that plays a specific game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showBoard():\n",
    "    '''Shows the board'''\n",
    "    for val in board:\n",
    "        print(val)\n",
    "\n",
    "def checkFree(x,y):\n",
    "    '''Takes an x,y postion and checks if that point on the board is free'''\n",
    "    if board[x,y] ==0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def checkWin(toggle):\n",
    "    '''checks wins for each player for diagonals, rows, columns'''\n",
    "    if toggle:\n",
    "        high = 6\n",
    "    else:\n",
    "        high = 15\n",
    "    if board.diagonal().sum() == high:\n",
    "        return True\n",
    "    if np.flip(board,1).diagonal().sum() == high:\n",
    "        return True\n",
    "    for val in range(0,3):\n",
    "        if board[val,:].sum()==high:\n",
    "            return True\n",
    "    for val in range(0,3):\n",
    "        if board[:,val].sum()==high:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getAvailablePositions():\n",
    "    pos = []\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if checkFree(i,j):\n",
    "                pos.append([i,j])\n",
    "    return pos\n",
    "\n",
    "def placePiece(x,y,nought_or_cross):\n",
    "    '''Takes a x, y position and a X or O with X=1, and O=2'''\n",
    "    board[x,y] = nought_or_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    '''Initilizes the q-values'''\n",
    "    q_val = {}\n",
    "    n_val = {}\n",
    "    moves = [[0,1],[0,2],[1,2],[2,2],[2,1],[0,0],[1,0],[1,1],[2,0]]\n",
    "    for val in moves:\n",
    "        q_val[str(val)]=0\n",
    "    for val in moves:\n",
    "        n_val[str(val)]=0\n",
    "    return q_val,n_val\n",
    "\n",
    "def predict(avail_moves,epsilon,q_val):\n",
    "    '''Takes the available moves, epsilon and q_values\n",
    "        Chooses a random move from the available moves if the random number is higher than epsilon. \n",
    "        Chooses the available move which has the highest q_val \n",
    "        Returns the move chosen\n",
    "    '''\n",
    "    ran_num = np.random.rand()\n",
    "    if ran_num > epsilon:\n",
    "        return avail_moves[np.random.randint(len(avail_moves))]\n",
    "    else:\n",
    "        max = q_val[str(avail_moves[0])]\n",
    "        for val in avail_moves:\n",
    "            if q_val[str(val)]>=max:\n",
    "                max = q_val[str(val)]\n",
    "                best = val\n",
    "        return best\n",
    "\n",
    "def updateQValues(movesPerformed,q_val,reward, n_values):\n",
    "    '''Updates q-values using the old q_vals, rewards and gameNum'''\n",
    "    for move in movesPerformed:\n",
    "        n = n_values[str(move)]\n",
    "        q_val[str(move)] = ((n-1)/float(n))*q_val[str(move)]+reward*(1/float(n))\n",
    "    return q_val\n",
    "\n",
    "def updateNValues(movesPerformed,n_val):\n",
    "    '''Updates q-values using the old n_vals, rewards and gameNum'''\n",
    "    for move in movesPerformed:\n",
    "        n_val[str(move)] = n_val[str(move)]+1\n",
    "    return n_val\n",
    "\n",
    "def reward(result,num_moves):\n",
    "    '''Reward a game won or lost'''\n",
    "    if result == 'lost':\n",
    "        return -10+(-num_moves)\n",
    "    elif result== 'win':\n",
    "        return 10+(-num_moves)\n",
    "    else:\n",
    "        return 0+(-num_moves)\n",
    "\n",
    "def chooseEpsilon(epoch):\n",
    "    return 1-np.exp(-epoch/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e06d3f2c18>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e06d374780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run function\n",
    "q_values, n_values = init()\n",
    "num_wins = []\n",
    "num_moves_per_game = []\n",
    "\n",
    "for epoch in range(1,1000):\n",
    "    \n",
    "    toggle = False\n",
    "    board = np.zeros((3,3))\n",
    "    #bad_expert = [[0,1],[0,2],[1,2],[2,2],[2,1],[0,0],[1,0],[1,1],[2,0]]\n",
    "    expert = [[1,1],[0,2],[2,0],[2,2],[0,0],[0,1],[1,0],[1,2],[2,1]]\n",
    "    count = 0\n",
    "    moves = []\n",
    "    epsilon = chooseEpsilon(epoch)\n",
    "    \n",
    "    while True:\n",
    "        toggle = not toggle\n",
    "        if toggle:\n",
    "            move = predict(getAvailablePositions(),epsilon,q_values)\n",
    "            x = move[0]\n",
    "            y = move[1]\n",
    "            moves.append(move)\n",
    "        else:\n",
    "            #use the current optimal policy - greedy strategy\n",
    "            move = predict(getAvailablePositions(),epsilon,q_values)\n",
    "            x = move[0]\n",
    "            y = move[1]\n",
    "        if toggle == True:\n",
    "            n_or_c = 5\n",
    "        else:\n",
    "            n_or_c = 2\n",
    "        placePiece(int(x),int(y),n_or_c)\n",
    "        if checkWin(toggle):\n",
    "            result = 'lost'\n",
    "            winner = -1\n",
    "            break\n",
    "        elif checkWin(not toggle):\n",
    "            result = 'win'\n",
    "            winner = 1\n",
    "            break\n",
    "        elif not getAvailablePositions():\n",
    "            result = 'draw'\n",
    "            winner = 0\n",
    "            break\n",
    "    n_values = updateNValues(moves,n_values)\n",
    "    q_values = updateQValues(moves,q_values, reward(result, len(moves)),n_values)\n",
    "    num_wins.append(winner)\n",
    "    num_moves_per_game.append(len(moves))\n",
    "    \n",
    "plt.plot(num_wins)\n",
    "plt.plot(num_moves_per_game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The q-values stabilize after about 35 runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 0]': 5.983068783068774,\n",
       " '[0, 1]': 1.4545454545454546,\n",
       " '[0, 2]': 5.93598233995585,\n",
       " '[1, 0]': 5.935969868173247,\n",
       " '[1, 1]': 5.994918699186985,\n",
       " '[1, 2]': 5.959999999999998,\n",
       " '[2, 0]': 5.959923664122137,\n",
       " '[2, 1]': 5.9833333333333325,\n",
       " '[2, 2]': 1.5454545454545454}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "agent moves\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  5.  0.]\n",
      "[ 0.  0.  0.]\n",
      "expert moves\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  5.  0.]\n",
      "[ 0.  2.  0.]\n",
      "agent moves\n",
      "[ 5.  0.  0.]\n",
      "[ 0.  5.  0.]\n",
      "[ 0.  2.  0.]\n",
      "expert moves\n",
      "[ 5.  0.  0.]\n",
      "[ 0.  5.  2.]\n",
      "[ 0.  2.  0.]\n",
      "agent moves\n",
      "[ 5.  0.  0.]\n",
      "[ 0.  5.  2.]\n",
      "[ 5.  2.  0.]\n",
      "expert moves\n",
      "[ 5.  0.  2.]\n",
      "[ 0.  5.  2.]\n",
      "[ 5.  2.  0.]\n",
      "agent moves\n",
      "5 has won!\n",
      "[ 5.  0.  2.]\n",
      "[ 5.  5.  2.]\n",
      "[ 5.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "#test function\n",
    "\n",
    "for epoch in range(1,2):\n",
    "    \n",
    "    toggle = False\n",
    "    board = np.zeros((3,3))\n",
    "    #bad_expert = [[0,1],[0,2],[1,2],[2,2],[2,1],[0,0],[1,0],[1,1],[2,0]]\n",
    "    expert = [[1,1],[0,2],[2,0],[2,2],[0,0],[0,1],[1,0],[1,2],[2,1]]\n",
    "    count = 0\n",
    "    moves = []\n",
    "    epsilon = chooseEpsilon(epoch)\n",
    "    \n",
    "    while True:\n",
    "        toggle = not toggle\n",
    "        #print(toggle)\n",
    "        showBoard()\n",
    "        if toggle:\n",
    "            print('agent moves')\n",
    "            move = predict(getAvailablePositions(),1,q_values)\n",
    "            x = move[0]\n",
    "            y = move[1]\n",
    "            moves.append(move)\n",
    "        else:\n",
    "            print('expert moves')\n",
    "            move = predict(getAvailablePositions(),1,q_values)\n",
    "            x = move[0]\n",
    "            y = move[1]\n",
    "        if toggle == True:\n",
    "            n_or_c = 5\n",
    "        else:\n",
    "            n_or_c = 2\n",
    "        placePiece(int(x),int(y),n_or_c)\n",
    "        if checkWin(toggle):\n",
    "            print('2 has won!')\n",
    "            winner = 0\n",
    "            break\n",
    "        elif checkWin(not toggle):\n",
    "            winner = 1\n",
    "            print('5 has won!')\n",
    "            break\n",
    "        elif not getAvailablePositions():\n",
    "            print('draw')\n",
    "            winner = -1\n",
    "            break\n",
    "    \n",
    "    showBoard() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The agent now plays the win in 3 moves. The q-values decide how fast it wins. It can vary from 3-4 from what i've seen. Of course the multiple trials are pointless because the q_values aren't changing over time. It would be interesting to see how the number of moves reduce over time when it is training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements will be to make a better expert, to test how much the agent can learn. The expert currently has not very good behavior. If we make the expert one that has a plan, will the agent be able to learn couteractive behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
